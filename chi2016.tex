\documentclass[a4paper]{article}

\usepackage{amsmath,amssymb,bm}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{floatrow}
\usepackage{blindtext}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}

\title{Quantifying Pains and Pleasures in Game, A Reinforcement Learning Approach}
\author{Bx. Wang}

\begin{document}
\maketitle

\section{Related Work}

\subsection{Online Game Player Motivation And Rewards}

One reason that online games appeal so many players is that it provides the environments for different kinds of play styles. The same online game may have respective meanings for different players. In the environment players gain certain kind of satisfactions (rewards) they want by conducting their actions. It's natural to assume a user's behaviors are highly correlated with the user's demanding satisfaction. And for game designers, finding the demanding satisfactions of users could make it easier to serve the users with better experience. 

Previous study categories the satisfactions into three major aspects, namely, achievement satisfactions, social satisfactions, and immersion satisfactions. It was then divided into ten subcategories, briefed in table. ~cite{tbl:satisfactions}. More detailes are introduced in ~\cite{} and ~\cite{}. The weights associated with these ten kinds of satisfactions basically chartered a user's profile, and it would be very meaningful the weights can be recovered in a purely data-driven process, based on the user's playing history.

\begin{tabular}{l|l|p{9cm}}
    \toprule
    Component  &Sub-component & Description \\
    \midrule
    Achievement & Advancement &  \\
     & Mechanics &  \\
     & Competition &  \\
    Social & Socializing &  \\
     & Relationship &  \\
     & Teamwork &  \\
    Immersion & Discovery &  \\
     & Role Playing &  \\
     & Customization &  \\
     & Escapism &  \\
    \bottomrule
    \label{tbe:satisfactions}
\end{tabular}

\section{Solution}

\subsection{Dataset Description}

To model the user behavior, we treat user as the agent who conduct an action every 10 minutes; during the 10-minute interval, the user decide the zone to stay in the next interval. If the user has been in multiple zones in a single interval, only the zone with longest stay duration was recorded. The actions are represented by zone IDs, ranging from 0 to 164, corresponding to 165 zones existing during Jan 2006 to Jan 2009 in WoW. When counting the index of interval, we ignore those minutes that the user's offline.

We use World of Warcraft Avatar History (WoWAH) dataset \cite{}, a dataset collected from realm \textit{TW-Light's Hope} during 1st Jan 2006 and 10th Jan 2009, containing 70,055 users after we filtering out those with too short playing history. Each user has spent 440.4 time intervals online on average. The dataset contains many different kinds of users: both novice and expert, guild members and isolated players, low-level and high-level players etc., with their respect class, and race in game. The detailed attributes are listed in Table~ref{tbl:attributes}.

During the gameplay the user is able to observe its current game states, including all the attributes recorded in the dataset. We construct the observation vector of the agent using the sequence of attributes extracted from its game plays in the most recent session (since the lastest log on). Instead of using the raw, concatenated vector of those attributes, we employ a preprocess to reduce the redundency, and make those decisive information more explicitly shown to the agent.

To model the user's behavior in a reinforcement learning perspective, we define the reward function which the agent tries to optimize during the gameplay. The reward is separated into five parts $r_1,\cdots r_5$, corresponding to five different kind of satisfactions defined in ~\cite{}. The construction of $r_1,\cdots r_5$ are illustrated in Table \ref{tbl:rewards}. Those rewards are normalized into the same scale.



\begin{tabular}{l|l|p{9cm}}
    \toprule
    Reward  &~Satisfaction~ & Definition \\
    \midrule
    $r^1$ & Advancement & The speed the user collecting experience in game \\
    $r^2$ & Competition & The satisfaction the user get by joining battleground or arena and competing with human opponents \\
    $r^3$ & Relationship & The long-term relationship with the user's guild, which is quantified by the time elapse since the user join its guild \\
    $r^4$ & Teamwork & The satisfaction the user get by playing in a zone which is featured by teamwork, e.g. Battleground, Arena, Dungeon, Raid, or a zone controlled by The Alliance. \\
    $r^5$ & Escapism & Escapism begins to cumulate if the user has been online for 4 hours or has been regularly login to the game for 20 days. \\
    \bottomrule
    \label{tbl:rewards}
\end{tabular}

\subsection{Reward Weight Recovery}

We apply inverse reinforcement learning, in order to recover the underlying reward mechanism of the players, using user's trajectory. Assume the total reward a user receives is the convex combination of the five rewards \ref{tbl:rewards}. Let $f_t=(r_t^1,r_t^2,r_t^3,r_t^4,r_t^5)$, we have the reward the user receives at time $t$ $$r_t=\phi^Tf_t,$$ 
where $\phi$ is the combination weight with $||\phi||_1=1$. Assume at each time step $t$, the user tries to take an action $a_t$ according to current game states so as to maximize the best expected cumulative discounted reward (known as the action-value function) $Q^\ast(s_t, a_t)$, where
$$Q^\ast(s,a)=\mathbf{E}[R_t | s_{t}=s, a_{t}=a | \pi^\ast]$$
and
$$R_t=\sum_{t^\prime\geq t}\gamma^{t^\prime-t}r_{t^\prime}=\sum_{t^\prime\geq t}\gamma^{t^\prime-t}\phi^Tf_{t^\prime}.$$
The term $\pi^\ast$ indicates optimal policy, featured by an distribution $\mathbb{P}(a|s)$ over the action space $\mathcal{A}(s)$. In this setting, the weight $\phi$ must satisfy that the action the user has taken must induce a larger $Q^\ast$ value than any other valid action would have done. Define $A(s)$ as the set of feasible action under state $s$, we have
$$Q^\ast(s,a) \geq \max_{a^\prime \in A(s)}Q^\ast(s,a^\prime) \label{eqn:irl}$$ 
satisfied for all $(s,a)$ pairs recorded in the user's trajectory. Consider the possible sub-optimal actions conducted by the users, we introduce slack variables $\xi_{s,a}$ into the problem formulation. Let $\xi_{s,a}$ be the difference of the actual action-value $Q^\ast(s,a)$, and the largest possible action-value $\max_{a^\prime \in A(s)}Q^\ast(s,a^\prime)$ whenever Eq. \eqref{eqn:irl} is not satisfied and zero otherwise, we minimize the summation of $\xi_{s,a}$ over the recorded user's trajectory


\begin{equation*}
\begin{aligned}
& \underset{\phi}{\text{minimize}}
& & C\sum_{s,a} \max(0, \max_{a^\prime \in A(s)\backslash a}Q^\ast(s,a^\prime) - Q^\ast(s,a))\\
& \text{subject to}
& & \phi \geq 0 \; \\
\label{eqn:raw_lp}
\end{aligned}
\end{equation*}


\subsection{Action-Value Function Approximation}

To solve \ref{eqn:lp} we have to be able to evaluate the $Q^*(s,a)$ value for every $(s,a)$ pair. Let
$$Q^i(s,a)=\mathbf{E}[\sum_{t^\prime\geq t}\gamma^{t^\prime-t}r^i_{t^\prime} | s_{t}=s, a_{t}=a | \pi^{i,\ast}]$$
be the action-value function, when the user only takes the $r^i$ into account and ignores all four other kinds of satisfactions. By definition we have $Q^*(s,a)=\phi^TQ(s,a)$, and it becomes suffice to the following linear programming, if $Q^i(s,a)$ can be evaluated for all $i$ and $(s,a)$ pairs in the trajectory.

\begin{equation*}
\begin{aligned}
& \underset{\phi^i, \xi_{s,a}}{\text{minimize}}
& & C\sum \xi_{s,a} \\
& \text{subject to}
& & \phi^T(Q(s,a)-Q(s,a^\prime)) \geq - \xi_{s,a}, \; \forall (s,a), a^\prime \in \mathcal{A}(s)\backslash a \\
&&& \phi \geq 0 \; \\
&&& \xi_{s,a} \geq 0 \; \forall (s,a).
\label{eqn:lp}
\end{aligned}
\end{equation*}

It suffice to solve $Q^i(s,a)$. As the number of feasible states $s$ could be arbitrary large, for an user in WoW, it's impossible to enumerate over the state space. Instead, we use deep-Q networks (DQN) \cite{} to approximate the $Q^i$ functions. The action-value function, by its definition, should satisfy the Bellman equation \cite{}, i.e. if the user takes action
$a$ and the state turns into $s_{t+1}$ from $s_t$,

$$Q^i(s,a)=r_{t} + \gamma \mathbf{E}_{a^\prime}[\max_{a^\prime}Q^i(s_{t+1}, a^\prime)].$$

$$L=(Q(s,a)-\gamma r_{t} + \mathbf{E}_{s_{t+1}}[\max_{a^\prime}Q(s_{t+1}, a^\prime)])^2$$

$$\theta -= \alpha\frac{\partial L}{\partial \theta}$$



$$ r(s)=\theta^T \phi(s)$$

$$ \max_{|\theta|_1=1} \quad C$$

$$ Q(s,a*) - max_{a\in A-a*}Q(s,a) > C \quad\forall s,a,a*$$




\end{document}
